# ğŸš€ Guide de dÃ©marrage rapide

Guide pas Ã  pas pour utiliser le DailyRH Scraper en 15 minutes.

## â±ï¸ Ã‰tape 1 : Installation (5 minutes)

```bash
# Naviguer dans le dossier du projet
cd dailyrh_scraper

# Installer les dÃ©pendances Python
pip install -r requirements.txt

# Installer le navigateur Chromium
playwright install chromium
```

âœ… **Installation terminÃ©e !**

## ğŸ” Ã‰tape 2 : Sauvegarder la session SSO (5 minutes)

```bash
python scripts/save_session.py
```

### Ce qu'il va se passer :

1. âœ… Un navigateur Chrome s'ouvre automatiquement
2. âœ… La page DailyRH se charge
3. ğŸ‘‰ **Vous devez vous connecter manuellement**
   - Entrez votre identifiant BNP Paribas
   - Entrez votre mot de passe  
   - Validez l'authentification Ã  deux facteurs si demandÃ©e
4. âœ… Une fois sur la page Team Planning, retournez au terminal
5. âœ… Appuyez sur **ENTRÃ‰E**
6. âœ… Le message "Session sauvegardÃ©e" apparaÃ®t

**âœ¨ Cette Ã©tape n'est Ã  faire qu'UNE SEULE FOIS !**

(ou lorsque la session expire aprÃ¨s quelques jours/semaines)

## â–¶ï¸ Ã‰tape 3 : Lancer le scraping (15-20 minutes)

```bash
python scripts/main.py
```

### Ce qu'il va se passer :

1. âœ… Chargement de DailyRH avec votre session
2. âœ… Navigation automatique vers janvier 2026
3. âœ… Extraction des donnÃ©es de **tous les mois** de l'annÃ©e
4. âœ… GÃ©nÃ©ration du CSV dans `output/`
5. âœ… GÃ©nÃ©ration de l'Excel dans `output/`
6. âœ… Message "Traitement terminÃ© avec succÃ¨s"

### Pendant le scraping

Vous verrez dÃ©filer des messages comme :
```
Traitement du mois : Janvier 2026
Nombre de collaborateurs : 25
Lignes extraites : 775
...
```

**Ne fermez pas le terminal pendant l'exÃ©cution !**

## ğŸ“ Ã‰tape 4 : RÃ©cupÃ©rer les fichiers

Les fichiers sont dans le dossier `output/` :

```
output/
â”œâ”€â”€ leave_planning_2026.csv         # DonnÃ©es brutes
â””â”€â”€ rapport_conges_2026.xlsx        # Rapport Excel
```

### Ouvrir l'Excel

Double-cliquez sur `rapport_conges_2026.xlsx` pour voir :

- **Feuille "SynthÃ¨se"** : Vue d'ensemble avec compteurs
- **Feuilles mensuelles** : Janvier, FÃ©vrier, Mars, ...
- **Feuilles individuelles** : Une par collaborateur

## ğŸ¯ Prochaines utilisations

La prochaine fois, il suffit d'exÃ©cuter :

```bash
python scripts/main.py
```

**C'est tout !** La session est dÃ©jÃ  sauvegardÃ©e. â˜•

## ğŸ¨ Comprendre les codes Excel

| Code | Signification |
|------|---------------|
| `CV` | CongÃ©s validÃ©s (journÃ©e entiÃ¨re) |
| `CP` | CongÃ©s Ã  valider |
| `TV` | TÃ©lÃ©travail validÃ© |
| `W` | Week-end ou jour fÃ©riÃ© |
| `CV-AM` | CongÃ©s le matin uniquement |
| `CV/TV` | CongÃ©s matin, tÃ©lÃ©travail aprÃ¨s-midi |

**Voir le README.md pour la liste complÃ¨te**

## âš™ï¸ Personnalisation rapide

### Changer l'annÃ©e

Ã‰ditez `src/config/config.py` :
```python
TARGET_YEAR = 2027  # Au lieu de 2026
```

### Mode invisible (sans navigateur)

Ã‰ditez `src/config/config.py` :
```python
HEADLESS_MODE = True  # Le navigateur ne s'affiche plus
```

### Plus de dÃ©tails dans les logs

Ã‰ditez `scripts/main.py` :
```python
logger = setup_logger(level="DEBUG")  # Au lieu de "INFO"
```

## â“ ProblÃ¨mes frÃ©quents

### âŒ "Session file not found"

**Cause** : Vous n'avez pas encore sauvegardÃ© la session

**Solution** :
```bash
python scripts/save_session.py
```

### âŒ "Authentication failed" ou "Login required"

**Cause** : La session a expirÃ©

**Solution** :
```bash
python scripts/save_session.py  # RafraÃ®chir la session
```

### âŒ Le scraping se bloque ou timeout

**Cause** : Les dÃ©lais sont trop courts

**Solution** : Augmentez les dÃ©lais dans `src/config/config.py` :
```python
NAVIGATION_DELAY = 3.0         # Au lieu de 1.5
INITIAL_LOAD_DELAY = 15        # Au lieu de 10
```

### âŒ DonnÃ©es manquantes pour certains mois

**Cause** : Erreur pendant le scraping d'un mois

**Solution** : 
1. Consultez `dailyrh_scraper.log`
2. Identifiez le mois problÃ©matique
3. RÃ©-exÃ©cutez le script

## ğŸ“– Aller plus loin

- **README.md** : Documentation complÃ¨te
- **docs/MODULES.md** : Comprendre l'architecture du code
- **dailyrh_scraper.log** : Journal dÃ©taillÃ© d'exÃ©cution

## âœ… RÃ©capitulatif

```bash
# Installation (une fois)
pip install -r requirements.txt
playwright install chromium

# Sauvegarder la session (une fois)
python scripts/save_session.py

# ExÃ©cuter le scraping (Ã  chaque fois)
python scripts/main.py

# RÃ©cupÃ©rer les fichiers
ls output/
```

**DurÃ©e totale** : ~25 minutes (premiÃ¨re fois), ~15 minutes (suivantes)

ğŸ‰ **C'est tout !** Vous Ãªtes prÃªt Ã  utiliser le scraper.
